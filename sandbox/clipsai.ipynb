{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/armeltalla/Documents/CAI/SDKs/clipsai/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = os.path.abspath(os.path.join(\"..\", \"data\"))\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file_path = os.path.join(DATA_PATH, \"test.mp4\")\n",
    "pyannote_auth_token = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipsai import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pyscenedetect:Downscale factor set to 5, effective resolution: 256 x 144\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "I0000 00:00:1704320767.611485       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crops:  [<clipsai.resize.segment.Segment object at 0x31e201bd0>, <clipsai.resize.segment.Segment object at 0x31e270110>]\n"
     ]
    }
   ],
   "source": [
    "crops = resize(\n",
    "    video_file_path=video_file_path,\n",
    "    pyannote_auth_token=pyannote_auth_token,\n",
    "    aspect_ratio=(9, 16)\n",
    ")\n",
    "\n",
    "print(\"Crops: \", crops.segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment(speakers: [0], start: 0.0, end: 13.047538, coordinates: (746, 0))\n",
      "Segment(speakers: [1], start: 13.047538, end: 38.933333, coordinates: (189, 0))\n"
     ]
    }
   ],
   "source": [
    "for segment in crops.segments:\n",
    "    print(segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipsai import transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.1.2. Bad things might happen unless you revert torch to 1.x.\n",
      "Detected language: en (0.99) in first 30s of audio...\n",
      "\n",
      "And if you do that, you've met that easy goal, and now you're in line with yourself, and that's going to reinforce you further. Exactly, exactly. And then when you also, there's another thing, I don't think you really hit a home on this to track your habits. Yeah, I almost forgot about that actually. I feel, you know, Ben, before we started this episode, Ben was like, dude, like, he even lasts episode. He just can probably borrow, like, tracking or habit start, like, when I, I was like, oh, like, when we say track habit, like, it doesn't mean the ironic thing to me about tracking habits is that it, it self is a habit. Like, so it's like, you have to start the habit of tracking your habit before you can actually really consistently track habit.\n"
     ]
    }
   ],
   "source": [
    "transcription = transcribe(media_file_path=video_file_path)\n",
    "print()\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clipsai import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip(start_time=0, end_time=38.856, start_char=0, end_char=753)\n"
     ]
    }
   ],
   "source": [
    "clips = clip(transcription=transcription)\n",
    "\n",
    "for clip in clips:\n",
    "    print(clip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
